{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "colab": {
      "name": "biobert_finetune_sts_biomedical_sentence_transformers.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JufH3ASp0kMJ",
        "colab_type": "code",
        "outputId": "447f991f-ac23-4652-8a1b-4ee55409caf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.83)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.35)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.18)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.11.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.18)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "y6hfRNET0kMZ",
        "colab_type": "code",
        "outputId": "936beaba-4169-4514-da56-e8397bc399a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: sentence-transformers in /usr/local/lib/python3.6/dist-packages (0.2.3)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: pytorch-transformers==1.1.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.21.3)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->sentence-transformers) (0.1.83)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->sentence-transformers) (2019.11.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->sentence-transformers) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->sentence-transformers) (1.10.18)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.14.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.1.0->sentence-transformers) (2019.9.11)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.1.0->sentence-transformers) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.1.0->sentence-transformers) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.1.0->sentence-transformers) (0.2.1)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.1.0->sentence-transformers) (1.13.18)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->pytorch-transformers==1.1.0->sentence-transformers) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->pytorch-transformers==1.1.0->sentence-transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIyu_U4A-zp1",
        "colab_type": "code",
        "outputId": "89ea598f-cdd6-4a4d-9bd2-15b3d9b237db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps_xhQUofjdJ",
        "colab_type": "code",
        "outputId": "88ae71ea-8518-429b-d43a-460f6ceed3bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXgSmDIRAZZh",
        "colab_type": "code",
        "outputId": "6b7c3a6e-63c3-4406-b3a4-b88f90fd40c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tgKRiLBw0kMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This examples trains BERT for the STSbenchmark from scratch. It generates sentence embeddings\n",
        "that can be compared using cosine-similarity to measure the similarity.\n",
        "\"\"\"\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "from sentence_transformers import SentenceTransformer,  SentencesDataset, LoggingHandler, losses, models\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers.readers import STSDataReader\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "#### Just some code to print debug information to stdout\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "                    level=logging.INFO,\n",
        "                    handlers=[LoggingHandler()])\n",
        "#### /print debug information to stdout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGBxRont-Ens",
        "colab_type": "code",
        "outputId": "2a6c0bc1-6f03-4088-c6a2-e870d7447cae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\n",
            "To: /content/biobert_v1.1_pubmed.tar.gz\n",
            "401MB [00:02, 172MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuMp_m-i-PAb",
        "colab_type": "code",
        "outputId": "7e80611f-f01d-4998-8b9e-12ee3385a411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "!tar xvzf biobert_v1.1_pubmed.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "biobert_v1.1_pubmed/\n",
            "biobert_v1.1_pubmed/model.ckpt-1000000.data-00000-of-00001\n",
            "biobert_v1.1_pubmed/model.ckpt-1000000.meta\n",
            "biobert_v1.1_pubmed/bert_config.json\n",
            "biobert_v1.1_pubmed/vocab.txt\n",
            "biobert_v1.1_pubmed/model.ckpt-1000000.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UQ3y9DEAFL5",
        "colab_type": "code",
        "outputId": "4283300e-9f56-42b0-ce3f-269d7ed1a2ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!transformers bert biobert_v1.1_pubmed/model.ckpt-1000000 biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/pytorch_model.bin"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building PyTorch model from configuration: {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_bert:Converting TensorFlow checkpoint from /content/biobert_v1.1_pubmed/model.ckpt-1000000\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/word_embeddings with shape [28996, 768]\n",
            "2019-12-04 09:18:53.856120: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 89075712 exceeds 10% of system memory.\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/pooler/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
            "Save PyTorch model to biobert_v1.1_pubmed/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NZewstSWjQF",
        "colab_type": "code",
        "outputId": "459eab25-e442-4887-9941-645bf08309e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7JblYqnWifM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#after this, renamed bert_config.json to config.json manually\n",
        "!cp biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/config.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mqEWk1v90kMh",
        "colab_type": "code",
        "outputId": "b5300605-da8a-44ef-c039-27a3b4e10cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "# Read the dataset\n",
        "''' 16 and 4 used for bert_stsb, bert_sts_biomedical and biobert_sts_biomedcial'''\n",
        "train_batch_size = 16\n",
        "num_epochs = 4\n",
        "\n",
        "\n",
        "\n",
        "model_save_path = 'output/training_n2c2_sts_bert-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "# parameters for STSDataReader -> dataset_folder, s1_col_idx=5, s2_col_idx=6, score_col_idx=4, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, normalize_scores=True, min_score=0, max_score=5\n",
        "sts_reader = STSDataReader('', s1_col_idx=0, s2_col_idx=1, score_col_idx=2, normalize_scores=True)\n",
        "\n",
        "\n",
        "\n",
        "'''# Map tokens to vectors using BERT\n",
        "word_embedding_model = models.BERT('biobert_v1.1_pubmed') #('bert-base-uncased')\n",
        "\n",
        "#cnn = models.CNN(in_word_embedding_dimension=word_embedding_model.get_word_embedding_dimension(), out_channels=256, kernel_sizes=[1,3,5])\n",
        "\n",
        "# Apply mean pooling to get one fixed sized sentence vector\n",
        "pooling_model = models.Pooling(cnn.get_word_embedding_dimension(),\n",
        "                               pooling_mode_mean_tokens=True,\n",
        "                               pooling_mode_cls_token=False,\n",
        "                               pooling_mode_max_tokens=False)\n",
        "\n",
        "\n",
        "model = SentenceTransformer(modules=[word_embedding_model, cnn, pooling_model])'''\n",
        "\n",
        "# Use BERT for mapping tokens to embeddings\n",
        "word_embedding_model = models.BERT('biobert_v1.1_pubmed') #('bert-base-uncased')\n",
        "\n",
        "# Apply mean pooling to get one fixed sized sentence vector\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
        "                               pooling_mode_mean_tokens=True,\n",
        "                               pooling_mode_cls_token=True,\n",
        "                               pooling_mode_max_tokens=True)\n",
        "\n",
        "#dense1 = models.Dense(in_features=3*word_embedding_model.get_word_embedding_dimension(), out_features=128)\n",
        "#dense2 = models.Dense(in_features=256, out_features=64)\n",
        "#dense3 = models.Dense(in_features=64, out_features=8)\n",
        "\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:   0%|          | 0/3 [00:48<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [00:48<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [00:48<?, ?it/s]\n",
            "Iteration:  24%|██▍       | 15/62 [00:48<00:40,  1.16it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:18:58 - loading configuration file biobert_v1.1_pubmed/config.json\n",
            "2019-12-04 09:18:58 - Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "2019-12-04 09:18:58 - loading weights file biobert_v1.1_pubmed/pytorch_model.bin\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:   0%|          | 0/3 [00:51<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [00:51<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [00:51<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [00:51<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [00:51<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [00:51<?, ?it/s]\n",
            "Iteration:  24%|██▍       | 15/62 [00:51<00:40,  1.16it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:19:00 - Model name 'biobert_v1.1_pubmed' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'biobert_v1.1_pubmed' is a path or url to a directory containing tokenizer files.\n",
            "2019-12-04 09:19:00 - Didn't find file biobert_v1.1_pubmed/added_tokens.json. We won't load it.\n",
            "2019-12-04 09:19:00 - Didn't find file biobert_v1.1_pubmed/special_tokens_map.json. We won't load it.\n",
            "2019-12-04 09:19:00 - loading file biobert_v1.1_pubmed/vocab.txt\n",
            "2019-12-04 09:19:00 - loading file None\n",
            "2019-12-04 09:19:00 - loading file None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:   0%|          | 0/3 [00:51<?, ?it/s]\n",
            "Iteration:  24%|██▍       | 15/62 [00:51<00:40,  1.16it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:19:01 - Use pytorch device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Kn7w48Ya0kMn",
        "colab_type": "code",
        "outputId": "e1698baf-7b25-4e38-8190-39e64c0e8b06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#sts_reader = STSDataReader('', s1_col_idx=0, s2_col_idx=1, score_col_idx=2, normalize_scores=True)\n",
        "\n",
        "# Convert the dataset to a DataLoader ready for training\n",
        "logging.info(\"Read STSbenchmark train dataset\")\n",
        "train_data = SentencesDataset(sts_reader.get_examples('clinicalSTS2019.train_train60.csv'), model)\n",
        "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
        "train_loss = losses.CosineSimilarityLoss(model=model)\n",
        "\n",
        "\n",
        "logging.info(\"Read STSbenchmark dev dataset\")\n",
        "dev_data = SentencesDataset(examples=sts_reader.get_examples('clinicalSTS2019.train_validate20.csv'), model=model)\n",
        "dev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=train_batch_size)\n",
        "evaluator = EmbeddingSimilarityEvaluator(dev_dataloader)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:   0%|          | 0/3 [00:51<?, ?it/s]\n",
            "Iteration:  24%|██▍       | 15/62 [00:51<00:40,  1.16it/s]\u001b[A\n",
            "\n",
            "Convert dataset:   0%|          | 0/992 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:19:01 - Read STSbenchmark train dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Convert dataset:   9%|▉         | 89/992 [00:00<00:01, 887.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert dataset:  17%|█▋        | 172/992 [00:00<00:00, 869.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert dataset:  26%|██▌       | 254/992 [00:00<00:00, 850.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert dataset:  34%|███▍      | 339/992 [00:00<00:00, 847.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert dataset:  43%|████▎     | 425/992 [00:00<00:00, 849.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert dataset:  52%|█████▏    | 513/992 [00:00<00:00, 857.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert dataset:  60%|██████    | 600/992 [00:00<00:00, 860.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert dataset:  68%|██████▊   | 679/992 [00:00<00:00, 814.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert dataset:  77%|███████▋  | 762/992 [00:00<00:00, 817.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert dataset:  86%|████████▌ | 855/992 [00:01<00:00, 847.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert dataset:  96%|█████████▌| 949/992 [00:01<00:00, 872.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Epoch:   0%|          | 0/3 [00:52<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [00:52<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [00:52<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [00:52<?, ?it/s]\n",
            "Iteration:  24%|██▍       | 15/62 [00:52<00:40,  1.16it/s]\u001b[A\n",
            "\n",
            "Convert dataset:   0%|          | 0/331 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert dataset:  14%|█▎        | 45/331 [00:00<00:00, 446.50it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:19:02 - Num sentences: 992\n",
            "2019-12-04 09:19:02 - Sentences 0 longer than max_seqence_length: 0\n",
            "2019-12-04 09:19:02 - Sentences 1 longer than max_seqence_length: 0\n",
            "2019-12-04 09:19:02 - Read STSbenchmark dev dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Convert dataset:  38%|███▊      | 125/331 [00:00<00:00, 514.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert dataset:  66%|██████▌   | 219/331 [00:00<00:00, 594.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert dataset:  93%|█████████▎| 307/331 [00:00<00:00, 657.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Epoch:   0%|          | 0/3 [00:53<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [00:53<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [00:53<?, ?it/s]\n",
            "Iteration:  24%|██▍       | 15/62 [00:53<00:40,  1.16it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:19:02 - Num sentences: 331\n",
            "2019-12-04 09:19:02 - Sentences 0 longer than max_seqence_length: 0\n",
            "2019-12-04 09:19:02 - Sentences 1 longer than max_seqence_length: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qB69vgWi0kMq",
        "colab_type": "code",
        "outputId": "1ef67163-db46-4681-a27d-ce9b47ca1a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Configure the training. We skip evaluation in this example\n",
        "warmup_steps = math.ceil(len(train_data)*num_epochs/train_batch_size*0.1) #10% of train data for warm-up\n",
        "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:   0%|          | 0/3 [00:53<?, ?it/s]\n",
            "Iteration:  24%|██▍       | 15/62 [00:53<00:40,  1.16it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:19:02 - Warmup-steps: 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eYRlcC2f0kMt",
        "colab_type": "code",
        "outputId": "33848738-29f4-42d8-d0c3-7ec2f93301c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          evaluator=evaluator,\n",
        "          epochs=num_epochs,\n",
        "          evaluation_steps=1000,\n",
        "          warmup_steps=warmup_steps,\n",
        "          output_path=model_save_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:   2%|▏         | 1/62 [00:01<01:11,  1.17s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:   3%|▎         | 2/62 [00:01<01:01,  1.03s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:   5%|▍         | 3/62 [00:02<00:59,  1.01s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:   6%|▋         | 4/62 [00:03<01:00,  1.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:   8%|▊         | 5/62 [00:04<00:55,  1.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  10%|▉         | 6/62 [00:06<00:59,  1.06s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  11%|█▏        | 7/62 [00:07<01:07,  1.22s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  13%|█▎        | 8/62 [00:08<00:57,  1.06s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  15%|█▍        | 9/62 [00:09<00:52,  1.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  16%|█▌        | 10/62 [00:10<00:54,  1.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  18%|█▊        | 11/62 [00:11<00:51,  1.00s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  19%|█▉        | 12/62 [00:11<00:45,  1.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  21%|██        | 13/62 [00:12<00:40,  1.20it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  23%|██▎       | 14/62 [00:13<00:44,  1.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  24%|██▍       | 15/62 [00:15<00:49,  1.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  26%|██▌       | 16/62 [00:15<00:45,  1.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  27%|██▋       | 17/62 [00:16<00:44,  1.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  29%|██▉       | 18/62 [00:17<00:38,  1.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  31%|███       | 19/62 [00:18<00:40,  1.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  32%|███▏      | 20/62 [00:19<00:37,  1.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  34%|███▍      | 21/62 [00:20<00:36,  1.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  35%|███▌      | 22/62 [00:21<00:38,  1.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  37%|███▋      | 23/62 [00:22<00:36,  1.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  39%|███▊      | 24/62 [00:23<00:36,  1.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  40%|████      | 25/62 [00:23<00:32,  1.15it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  42%|████▏     | 26/62 [00:25<00:34,  1.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  44%|████▎     | 27/62 [00:26<00:35,  1.01s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  45%|████▌     | 28/62 [00:27<00:34,  1.01s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  47%|████▋     | 29/62 [00:28<00:33,  1.01s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  48%|████▊     | 30/62 [00:29<00:33,  1.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  50%|█████     | 31/62 [00:30<00:35,  1.14s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  52%|█████▏    | 32/62 [00:32<00:36,  1.21s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  53%|█████▎    | 33/62 [00:32<00:31,  1.09s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  55%|█████▍    | 34/62 [00:34<00:35,  1.25s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  56%|█████▋    | 35/62 [00:35<00:34,  1.28s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  58%|█████▊    | 36/62 [00:37<00:34,  1.33s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  60%|█████▉    | 37/62 [00:38<00:28,  1.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  61%|██████▏   | 38/62 [00:38<00:24,  1.03s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  63%|██████▎   | 39/62 [00:40<00:25,  1.09s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  65%|██████▍   | 40/62 [00:41<00:24,  1.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  66%|██████▌   | 41/62 [00:42<00:22,  1.06s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  68%|██████▊   | 42/62 [00:43<00:21,  1.07s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  69%|██████▉   | 43/62 [00:44<00:19,  1.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  71%|███████   | 44/62 [00:45<00:18,  1.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  73%|███████▎  | 45/62 [00:46<00:16,  1.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  74%|███████▍  | 46/62 [00:46<00:14,  1.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  76%|███████▌  | 47/62 [00:47<00:13,  1.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  77%|███████▋  | 48/62 [00:49<00:14,  1.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  79%|███████▉  | 49/62 [00:49<00:12,  1.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  81%|████████  | 50/62 [00:50<00:10,  1.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  82%|████████▏ | 51/62 [00:51<00:10,  1.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  84%|████████▍ | 52/62 [00:52<00:09,  1.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  85%|████████▌ | 53/62 [00:53<00:09,  1.07s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  87%|████████▋ | 54/62 [00:54<00:08,  1.01s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  89%|████████▊ | 55/62 [00:56<00:07,  1.10s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  90%|█████████ | 56/62 [00:57<00:07,  1.26s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  92%|█████████▏| 57/62 [00:58<00:05,  1.16s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  94%|█████████▎| 58/62 [00:59<00:04,  1.06s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  95%|█████████▌| 59/62 [01:00<00:02,  1.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  97%|█████████▋| 60/62 [01:01<00:02,  1.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  98%|█████████▊| 61/62 [01:02<00:00,  1.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration: 100%|██████████| 62/62 [01:03<00:00,  1.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Epoch:   0%|          | 0/3 [01:56<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [01:03<?, ?it/s]\u001b[A\u001b[A\n",
            "Iteration:  24%|██▍       | 15/62 [01:56<00:40,  1.16it/s]\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:   0%|          | 0/21 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:20:06 - Evaluation the model on  dataset after epoch 0:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Convert Evaluating:   5%|▍         | 1/21 [00:00<00:04,  4.16it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  10%|▉         | 2/21 [00:00<00:06,  3.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  14%|█▍        | 3/21 [00:01<00:06,  2.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  19%|█▉        | 4/21 [00:01<00:06,  2.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  24%|██▍       | 5/21 [00:01<00:05,  3.15it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  29%|██▊       | 6/21 [00:02<00:04,  3.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  33%|███▎      | 7/21 [00:02<00:05,  2.56it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  38%|███▊      | 8/21 [00:03<00:05,  2.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  43%|████▎     | 9/21 [00:03<00:04,  2.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  48%|████▊     | 10/21 [00:03<00:03,  2.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  52%|█████▏    | 11/21 [00:03<00:03,  3.19it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  57%|█████▋    | 12/21 [00:04<00:02,  3.18it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  62%|██████▏   | 13/21 [00:04<00:02,  3.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  67%|██████▋   | 14/21 [00:04<00:02,  3.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  71%|███████▏  | 15/21 [00:05<00:02,  2.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  76%|███████▌  | 16/21 [00:05<00:01,  2.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  81%|████████  | 17/21 [00:06<00:01,  2.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  86%|████████▌ | 18/21 [00:06<00:01,  2.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  90%|█████████ | 19/21 [00:06<00:00,  2.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  95%|█████████▌| 20/21 [00:06<00:00,  3.25it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating: 100%|██████████| 21/21 [00:07<00:00,  3.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Epoch:   0%|          | 0/3 [02:03<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [01:10<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Epoch:   0%|          | 0/3 [02:03<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [01:10<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Epoch:   0%|          | 0/3 [02:03<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [01:10<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Epoch:   0%|          | 0/3 [02:03<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [01:10<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Epoch:   0%|          | 0/3 [02:03<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [01:10<?, ?it/s]\u001b[A\u001b[A\n",
            "Iteration:  24%|██▍       | 15/62 [02:03<00:40,  1.16it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:20:13 - Cosine-Similarity :\tPearson: 0.7412\tSpearman: 0.7425\n",
            "2019-12-04 09:20:13 - Manhattan-Distance:\tPearson: 0.7138\tSpearman: 0.7346\n",
            "2019-12-04 09:20:13 - Euclidean-Distance:\tPearson: 0.7154\tSpearman: 0.7352\n",
            "2019-12-04 09:20:13 - Dot-Product-Similarity:\tPearson: 0.7419\tSpearman: 0.7379\n",
            "2019-12-04 09:20:13 - Save model to output/training_n2c2_sts_bert-2019-12-04_09-18-58\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch:  33%|███▎      | 1/3 [01:11<02:23, 71.84s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\n",
            "Epoch:   0%|          | 0/3 [02:05<?, ?it/s]\n",
            "\n",
            "Epoch:  33%|███▎      | 1/3 [01:11<02:23, 71.84s/it]\u001b[A\u001b[A\n",
            "Iteration:  24%|██▍       | 15/62 [02:05<00:40,  1.16it/s]\u001b[A\n",
            "\n",
            "\n",
            "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:20:14 - Restart data_iterator\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration:   2%|▏         | 1/62 [00:00<00:52,  1.15it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:   3%|▎         | 2/62 [00:01<00:49,  1.21it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:   5%|▍         | 3/62 [00:02<00:50,  1.16it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:   6%|▋         | 4/62 [00:03<00:55,  1.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:   8%|▊         | 5/62 [00:04<00:58,  1.03s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  10%|▉         | 6/62 [00:06<01:04,  1.15s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  11%|█▏        | 7/62 [00:07<01:08,  1.24s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  13%|█▎        | 8/62 [00:09<01:12,  1.35s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  15%|█▍        | 9/62 [00:10<01:05,  1.23s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  16%|█▌        | 10/62 [00:11<01:05,  1.26s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  18%|█▊        | 11/62 [00:12<01:02,  1.22s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  19%|█▉        | 12/62 [00:13<00:55,  1.11s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  21%|██        | 13/62 [00:14<00:48,  1.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  23%|██▎       | 14/62 [00:15<00:47,  1.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  24%|██▍       | 15/62 [00:16<00:49,  1.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  26%|██▌       | 16/62 [00:17<00:43,  1.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  27%|██▋       | 17/62 [00:18<00:49,  1.10s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  29%|██▉       | 18/62 [00:19<00:49,  1.14s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  31%|███       | 19/62 [00:21<00:51,  1.19s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  32%|███▏      | 20/62 [00:22<00:48,  1.15s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  34%|███▍      | 21/62 [00:23<00:47,  1.17s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  35%|███▌      | 22/62 [00:24<00:50,  1.26s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  37%|███▋      | 23/62 [00:26<00:47,  1.22s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  39%|███▊      | 24/62 [00:27<00:44,  1.16s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  40%|████      | 25/62 [00:27<00:39,  1.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  42%|████▏     | 26/62 [00:29<00:39,  1.09s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  44%|████▎     | 27/62 [00:29<00:35,  1.01s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  45%|████▌     | 28/62 [00:31<00:35,  1.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  47%|████▋     | 29/62 [00:32<00:40,  1.22s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  48%|████▊     | 30/62 [00:33<00:35,  1.10s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  50%|█████     | 31/62 [00:34<00:30,  1.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  52%|█████▏    | 32/62 [00:35<00:32,  1.09s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  53%|█████▎    | 33/62 [00:36<00:28,  1.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  55%|█████▍    | 34/62 [00:37<00:24,  1.13it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  56%|█████▋    | 35/62 [00:37<00:24,  1.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  58%|█████▊    | 36/62 [00:39<00:25,  1.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  60%|█████▉    | 37/62 [00:40<00:23,  1.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  61%|██████▏   | 38/62 [00:41<00:24,  1.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  63%|██████▎   | 39/62 [00:42<00:23,  1.01s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  65%|██████▍   | 40/62 [00:43<00:21,  1.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  66%|██████▌   | 41/62 [00:43<00:19,  1.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  68%|██████▊   | 42/62 [00:45<00:22,  1.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  69%|██████▉   | 43/62 [00:46<00:20,  1.09s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  71%|███████   | 44/62 [00:47<00:19,  1.07s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  73%|███████▎  | 45/62 [00:48<00:17,  1.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  74%|███████▍  | 46/62 [00:49<00:17,  1.07s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  76%|███████▌  | 47/62 [00:50<00:14,  1.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  77%|███████▋  | 48/62 [00:51<00:14,  1.01s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  79%|███████▉  | 49/62 [00:52<00:12,  1.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  81%|████████  | 50/62 [00:53<00:11,  1.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  82%|████████▏ | 51/62 [00:53<00:09,  1.18it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  84%|████████▍ | 52/62 [00:54<00:08,  1.13it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  85%|████████▌ | 53/62 [00:55<00:08,  1.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  87%|████████▋ | 54/62 [00:56<00:06,  1.16it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  89%|████████▊ | 55/62 [00:57<00:06,  1.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  90%|█████████ | 56/62 [00:58<00:06,  1.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  92%|█████████▏| 57/62 [00:59<00:04,  1.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  94%|█████████▎| 58/62 [01:00<00:03,  1.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  95%|█████████▌| 59/62 [01:01<00:02,  1.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  97%|█████████▋| 60/62 [01:02<00:01,  1.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  98%|█████████▊| 61/62 [01:03<00:01,  1.07s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration: 100%|██████████| 62/62 [01:04<00:00,  1.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Epoch:   0%|          | 0/3 [03:09<?, ?it/s]\n",
            "\n",
            "Epoch:  33%|███▎      | 1/3 [02:16<02:23, 71.84s/it]\u001b[A\u001b[A\n",
            "Iteration:  24%|██▍       | 15/62 [03:09<00:40,  1.16it/s]\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:   0%|          | 0/21 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:21:19 - Evaluation the model on  dataset after epoch 1:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Convert Evaluating:   5%|▍         | 1/21 [00:00<00:04,  4.20it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  10%|▉         | 2/21 [00:00<00:06,  3.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  14%|█▍        | 3/21 [00:01<00:06,  2.70it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  19%|█▉        | 4/21 [00:01<00:06,  2.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  24%|██▍       | 5/21 [00:01<00:05,  3.18it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  29%|██▊       | 6/21 [00:02<00:04,  3.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  33%|███▎      | 7/21 [00:02<00:05,  2.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  38%|███▊      | 8/21 [00:03<00:05,  2.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  43%|████▎     | 9/21 [00:03<00:04,  2.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  48%|████▊     | 10/21 [00:03<00:03,  2.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  52%|█████▏    | 11/21 [00:03<00:03,  3.19it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  57%|█████▋    | 12/21 [00:04<00:02,  3.16it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  62%|██████▏   | 13/21 [00:04<00:02,  3.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  67%|██████▋   | 14/21 [00:04<00:02,  3.11it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  71%|███████▏  | 15/21 [00:05<00:02,  2.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  76%|███████▌  | 16/21 [00:05<00:01,  2.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  81%|████████  | 17/21 [00:06<00:01,  2.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  86%|████████▌ | 18/21 [00:06<00:01,  2.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  90%|█████████ | 19/21 [00:06<00:00,  2.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  95%|█████████▌| 20/21 [00:06<00:00,  3.27it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating: 100%|██████████| 21/21 [00:07<00:00,  3.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Epoch:   0%|          | 0/3 [03:17<?, ?it/s]\n",
            "\n",
            "Epoch:  33%|███▎      | 1/3 [02:23<02:23, 71.84s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Epoch:   0%|          | 0/3 [03:17<?, ?it/s]\n",
            "\n",
            "Epoch:  33%|███▎      | 1/3 [02:23<02:23, 71.84s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Epoch:   0%|          | 0/3 [03:17<?, ?it/s]\n",
            "\n",
            "Epoch:  33%|███▎      | 1/3 [02:23<02:23, 71.84s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Epoch:   0%|          | 0/3 [03:17<?, ?it/s]\n",
            "\n",
            "Epoch:  33%|███▎      | 1/3 [02:23<02:23, 71.84s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Epoch:   0%|          | 0/3 [03:17<?, ?it/s]\n",
            "\n",
            "Epoch:  33%|███▎      | 1/3 [02:23<02:23, 71.84s/it]\u001b[A\u001b[A\n",
            "Iteration:  24%|██▍       | 15/62 [03:17<00:40,  1.16it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:21:26 - Cosine-Similarity :\tPearson: 0.7666\tSpearman: 0.7708\n",
            "2019-12-04 09:21:26 - Manhattan-Distance:\tPearson: 0.7420\tSpearman: 0.7674\n",
            "2019-12-04 09:21:26 - Euclidean-Distance:\tPearson: 0.7410\tSpearman: 0.7652\n",
            "2019-12-04 09:21:26 - Dot-Product-Similarity:\tPearson: 0.7680\tSpearman: 0.7696\n",
            "2019-12-04 09:21:26 - Save model to output/training_n2c2_sts_bert-2019-12-04_09-18-58\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch:  67%|██████▋   | 2/3 [02:25<01:12, 72.31s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\n",
            "Epoch:   0%|          | 0/3 [03:18<?, ?it/s]\n",
            "\n",
            "Epoch:  67%|██████▋   | 2/3 [02:25<01:12, 72.31s/it]\u001b[A\u001b[A\n",
            "Iteration:  24%|██▍       | 15/62 [03:18<00:40,  1.16it/s]\u001b[A\n",
            "\n",
            "\n",
            "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:21:28 - Restart data_iterator\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration:   2%|▏         | 1/62 [00:00<00:45,  1.34it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:   3%|▎         | 2/62 [00:01<00:44,  1.34it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:   5%|▍         | 3/62 [00:02<00:44,  1.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:   6%|▋         | 4/62 [00:03<00:47,  1.23it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:   8%|▊         | 5/62 [00:04<00:57,  1.00s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  10%|▉         | 6/62 [00:05<00:52,  1.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  11%|█▏        | 7/62 [00:06<00:55,  1.01s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  13%|█▎        | 8/62 [00:07<00:53,  1.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  15%|█▍        | 9/62 [00:08<00:51,  1.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  16%|█▌        | 10/62 [00:09<00:53,  1.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  18%|█▊        | 11/62 [00:10<00:50,  1.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  19%|█▉        | 12/62 [00:11<00:49,  1.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  21%|██        | 13/62 [00:12<00:45,  1.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  23%|██▎       | 14/62 [00:13<00:44,  1.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  24%|██▍       | 15/62 [00:14<00:43,  1.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  26%|██▌       | 16/62 [00:15<00:49,  1.07s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  27%|██▋       | 17/62 [00:16<00:50,  1.11s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  29%|██▉       | 18/62 [00:17<00:42,  1.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  31%|███       | 19/62 [00:19<00:49,  1.15s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  32%|███▏      | 20/62 [00:20<00:47,  1.12s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  34%|███▍      | 21/62 [00:20<00:42,  1.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  35%|███▌      | 22/62 [00:21<00:38,  1.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  37%|███▋      | 23/62 [00:22<00:39,  1.00s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  39%|███▊      | 24/62 [00:23<00:40,  1.06s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  40%|████      | 25/62 [00:24<00:36,  1.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  42%|████▏     | 26/62 [00:25<00:37,  1.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  44%|████▎     | 27/62 [00:26<00:32,  1.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  45%|████▌     | 28/62 [00:27<00:35,  1.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  47%|████▋     | 29/62 [00:29<00:38,  1.18s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  48%|████▊     | 30/62 [00:30<00:37,  1.17s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  50%|█████     | 31/62 [00:31<00:32,  1.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  52%|█████▏    | 32/62 [00:32<00:32,  1.08s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  53%|█████▎    | 33/62 [00:33<00:30,  1.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  55%|█████▍    | 34/62 [00:34<00:29,  1.06s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  56%|█████▋    | 35/62 [00:35<00:28,  1.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  58%|█████▊    | 36/62 [00:36<00:26,  1.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  60%|█████▉    | 37/62 [00:37<00:26,  1.08s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  61%|██████▏   | 38/62 [00:38<00:26,  1.11s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  63%|██████▎   | 39/62 [00:40<00:27,  1.18s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  65%|██████▍   | 40/62 [00:41<00:23,  1.06s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  66%|██████▌   | 41/62 [00:42<00:22,  1.08s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  68%|██████▊   | 42/62 [00:43<00:23,  1.15s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  69%|██████▉   | 43/62 [00:44<00:21,  1.15s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  71%|███████   | 44/62 [00:45<00:21,  1.20s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  73%|███████▎  | 45/62 [00:46<00:18,  1.07s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  74%|███████▍  | 46/62 [00:47<00:16,  1.01s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  76%|███████▌  | 47/62 [00:48<00:15,  1.06s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  77%|███████▋  | 48/62 [00:49<00:14,  1.03s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  79%|███████▉  | 49/62 [00:50<00:14,  1.09s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  81%|████████  | 50/62 [00:52<00:13,  1.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  82%|████████▏ | 51/62 [00:52<00:11,  1.01s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  84%|████████▍ | 52/62 [00:53<00:10,  1.01s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  85%|████████▌ | 53/62 [00:54<00:08,  1.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  87%|████████▋ | 54/62 [00:56<00:09,  1.16s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  89%|████████▊ | 55/62 [00:57<00:08,  1.16s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  90%|█████████ | 56/62 [00:58<00:06,  1.09s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  92%|█████████▏| 57/62 [00:59<00:04,  1.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  94%|█████████▎| 58/62 [01:00<00:03,  1.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  95%|█████████▌| 59/62 [01:00<00:02,  1.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  97%|█████████▋| 60/62 [01:02<00:02,  1.00s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration:  98%|█████████▊| 61/62 [01:03<00:00,  1.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Iteration: 100%|██████████| 62/62 [01:04<00:00,  1.17s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Epoch:   0%|          | 0/3 [04:23<?, ?it/s]\n",
            "\n",
            "Epoch:  67%|██████▋   | 2/3 [03:29<01:12, 72.31s/it]\u001b[A\u001b[A\n",
            "Iteration:  24%|██▍       | 15/62 [04:23<00:40,  1.16it/s]\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:   0%|          | 0/21 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:22:32 - Evaluation the model on  dataset after epoch 2:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Convert Evaluating:   5%|▍         | 1/21 [00:00<00:04,  4.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  10%|▉         | 2/21 [00:00<00:06,  3.13it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  14%|█▍        | 3/21 [00:01<00:06,  2.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  19%|█▉        | 4/21 [00:01<00:06,  2.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  24%|██▍       | 5/21 [00:01<00:05,  3.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  29%|██▊       | 6/21 [00:02<00:04,  3.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  33%|███▎      | 7/21 [00:02<00:05,  2.56it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  38%|███▊      | 8/21 [00:03<00:05,  2.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  43%|████▎     | 9/21 [00:03<00:04,  2.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  48%|████▊     | 10/21 [00:03<00:03,  2.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  52%|█████▏    | 11/21 [00:03<00:03,  3.20it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  57%|█████▋    | 12/21 [00:04<00:02,  3.18it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  62%|██████▏   | 13/21 [00:04<00:02,  3.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  67%|██████▋   | 14/21 [00:04<00:02,  3.15it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  71%|███████▏  | 15/21 [00:05<00:02,  2.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  76%|███████▌  | 16/21 [00:05<00:01,  2.61it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  81%|████████  | 17/21 [00:06<00:01,  2.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  86%|████████▌ | 18/21 [00:06<00:01,  2.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  90%|█████████ | 19/21 [00:06<00:00,  2.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating:  95%|█████████▌| 20/21 [00:06<00:00,  3.26it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Convert Evaluating: 100%|██████████| 21/21 [00:07<00:00,  3.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Epoch:   0%|          | 0/3 [04:30<?, ?it/s]\n",
            "\n",
            "Epoch:  67%|██████▋   | 2/3 [03:37<01:12, 72.31s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Epoch:   0%|          | 0/3 [04:30<?, ?it/s]\n",
            "\n",
            "Epoch:  67%|██████▋   | 2/3 [03:37<01:12, 72.31s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Epoch:   0%|          | 0/3 [04:30<?, ?it/s]\n",
            "\n",
            "Epoch:  67%|██████▋   | 2/3 [03:37<01:12, 72.31s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Epoch:   0%|          | 0/3 [04:30<?, ?it/s]\n",
            "\n",
            "Epoch:  67%|██████▋   | 2/3 [03:37<01:12, 72.31s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Epoch:   0%|          | 0/3 [04:30<?, ?it/s]\n",
            "\n",
            "Epoch:  67%|██████▋   | 2/3 [03:37<01:12, 72.31s/it]\u001b[A\u001b[A\n",
            "Iteration:  24%|██▍       | 15/62 [04:30<00:40,  1.16it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:22:40 - Cosine-Similarity :\tPearson: 0.7708\tSpearman: 0.7736\n",
            "2019-12-04 09:22:40 - Manhattan-Distance:\tPearson: 0.7447\tSpearman: 0.7678\n",
            "2019-12-04 09:22:40 - Euclidean-Distance:\tPearson: 0.7443\tSpearman: 0.7678\n",
            "2019-12-04 09:22:40 - Dot-Product-Similarity:\tPearson: 0.7724\tSpearman: 0.7736\n",
            "2019-12-04 09:22:40 - Save model to output/training_n2c2_sts_bert-2019-12-04_09-18-58\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch: 100%|██████████| 3/3 [03:38<00:00, 72.51s/it]\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7DzTfukm0kMw",
        "colab_type": "code",
        "outputId": "6784bdc1-843b-42cc-bfef-902fd0d5c3d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "##############################################################################\n",
        "#\n",
        "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
        "#\n",
        "##############################################################################\n",
        "\n",
        "model = SentenceTransformer(model_save_path)\n",
        "test_data = SentencesDataset(examples=sts_reader.get_examples(\"clinicalSTS2019.train_test20.csv\"), model=model)\n",
        "test_dataloader = DataLoader(test_data, shuffle=False, batch_size=train_batch_size)\n",
        "evaluator = EmbeddingSimilarityEvaluator(test_dataloader)\n",
        "model.evaluate(evaluator)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:   0%|          | 0/3 [04:31<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [04:31<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [04:31<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [04:31<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [04:31<?, ?it/s]\n",
            "Iteration:  24%|██▍       | 15/62 [04:31<00:40,  1.16it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:22:41 - Load pretrained SentenceTransformer: output/training_n2c2_sts_bert-2019-12-04_09-18-58\n",
            "2019-12-04 09:22:41 - Load SentenceTransformer from folder: output/training_n2c2_sts_bert-2019-12-04_09-18-58\n",
            "2019-12-04 09:22:41 - loading configuration file output/training_n2c2_sts_bert-2019-12-04_09-18-58/0_BERT/config.json\n",
            "2019-12-04 09:22:41 - Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "2019-12-04 09:22:41 - loading weights file output/training_n2c2_sts_bert-2019-12-04_09-18-58/0_BERT/pytorch_model.bin\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:   0%|          | 0/3 [04:33<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [04:33<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [04:33<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [04:33<?, ?it/s]\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:22:43 - Model name 'output/training_n2c2_sts_bert-2019-12-04_09-18-58/0_BERT' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'output/training_n2c2_sts_bert-2019-12-04_09-18-58/0_BERT' is a path or url to a directory containing tokenizer files.\n",
            "2019-12-04 09:22:43 - loading file output/training_n2c2_sts_bert-2019-12-04_09-18-58/0_BERT/vocab.txt\n",
            "2019-12-04 09:22:43 - loading file output/training_n2c2_sts_bert-2019-12-04_09-18-58/0_BERT/added_tokens.json\n",
            "2019-12-04 09:22:43 - loading file output/training_n2c2_sts_bert-2019-12-04_09-18-58/0_BERT/special_tokens_map.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/3 [04:34<?, ?it/s]\n",
            "Iteration:  24%|██▍       | 15/62 [04:34<00:40,  1.16it/s]\u001b[A\n",
            "\n",
            "Convert dataset:   0%|          | 0/331 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:22:43 - Use pytorch device: cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Convert dataset:  28%|██▊       | 94/331 [00:00<00:00, 939.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert dataset:  58%|█████▊    | 191/331 [00:00<00:00, 945.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert dataset:  86%|████████▌ | 285/331 [00:00<00:00, 942.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Epoch:   0%|          | 0/3 [04:34<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [04:34<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [04:34<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [04:34<?, ?it/s]\n",
            "Iteration:  24%|██▍       | 15/62 [04:34<00:40,  1.16it/s]\u001b[A\n",
            "\n",
            "Convert Evaluating:   0%|          | 0/21 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:22:44 - Num sentences: 331\n",
            "2019-12-04 09:22:44 - Sentences 0 longer than max_seqence_length: 0\n",
            "2019-12-04 09:22:44 - Sentences 1 longer than max_seqence_length: 0\n",
            "2019-12-04 09:22:44 - Evaluation the model on  dataset:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Convert Evaluating:   5%|▍         | 1/21 [00:00<00:05,  3.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  10%|▉         | 2/21 [00:00<00:06,  3.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  14%|█▍        | 3/21 [00:01<00:06,  2.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  19%|█▉        | 4/21 [00:01<00:05,  3.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  24%|██▍       | 5/21 [00:01<00:05,  2.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  29%|██▊       | 6/21 [00:02<00:04,  3.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  33%|███▎      | 7/21 [00:02<00:04,  2.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  38%|███▊      | 8/21 [00:02<00:04,  2.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  43%|████▎     | 9/21 [00:02<00:03,  3.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  48%|████▊     | 10/21 [00:03<00:03,  2.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  52%|█████▏    | 11/21 [00:03<00:03,  2.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  57%|█████▋    | 12/21 [00:04<00:03,  2.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  62%|██████▏   | 13/21 [00:04<00:02,  3.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  67%|██████▋   | 14/21 [00:04<00:02,  3.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  71%|███████▏  | 15/21 [00:04<00:01,  3.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  76%|███████▌  | 16/21 [00:05<00:01,  3.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  81%|████████  | 17/21 [00:05<00:01,  3.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  86%|████████▌ | 18/21 [00:05<00:00,  3.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  90%|█████████ | 19/21 [00:06<00:00,  3.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating:  95%|█████████▌| 20/21 [00:06<00:00,  3.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Convert Evaluating: 100%|██████████| 21/21 [00:06<00:00,  4.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Epoch:   0%|          | 0/3 [04:41<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [04:41<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [04:41<?, ?it/s]\n",
            "\n",
            "Epoch:   0%|          | 0/3 [04:41<?, ?it/s]\n",
            "Iteration:  24%|██▍       | 15/62 [04:41<00:40,  1.16it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-12-04 09:22:50 - Cosine-Similarity :\tPearson: 0.7379\tSpearman: 0.7280\n",
            "2019-12-04 09:22:50 - Manhattan-Distance:\tPearson: 0.7124\tSpearman: 0.7284\n",
            "2019-12-04 09:22:50 - Euclidean-Distance:\tPearson: 0.7098\tSpearman: 0.7279\n",
            "2019-12-04 09:22:50 - Dot-Product-Similarity:\tPearson: 0.7347\tSpearman: 0.7192\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7284098353176021"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsee_LAZATg5",
        "colab_type": "code",
        "outputId": "a352cb3a-c315-4e18-af02-8abb6eb6e301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!zip -r output.zip output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: output/ (stored 0%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/ (stored 0%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/2_Dense/ (stored 0%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/2_Dense/pytorch_model.bin (deflated 8%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/2_Dense/config.json (deflated 22%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/similarity_evaluation_results.csv (deflated 49%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/3_Dense/ (stored 0%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/3_Dense/pytorch_model.bin (deflated 8%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/3_Dense/config.json (deflated 22%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/config.json (stored 0%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/modules.json (deflated 73%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/1_Pooling/ (stored 0%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/1_Pooling/config.json (deflated 49%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/0_BERT/ (stored 0%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/0_BERT/added_tokens.json (stored 0%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/0_BERT/sentence_bert_config.json (deflated 4%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/0_BERT/vocab.txt (deflated 49%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/0_BERT/pytorch_model.bin (deflated 7%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/0_BERT/config.json (deflated 51%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/0_BERT/special_tokens_map.json (deflated 40%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/4_Dense/ (stored 0%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/4_Dense/pytorch_model.bin (deflated 9%)\n",
            "updating: output/training_n2c2_sts_bert-2019-12-04_08-34-47/4_Dense/config.json (deflated 23%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-04-35/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-04-35/2_Dense/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-04-35/2_Dense/pytorch_model.bin (deflated 8%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-04-35/2_Dense/config.json (deflated 22%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-04-35/similarity_evaluation_results.csv (deflated 51%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-04-35/config.json (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-04-35/modules.json (deflated 61%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-04-35/1_Pooling/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-04-35/1_Pooling/config.json (deflated 47%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-04-35/0_BERT/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-04-35/0_BERT/added_tokens.json (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-04-35/0_BERT/sentence_bert_config.json (deflated 4%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-04-35/0_BERT/vocab.txt (deflated 49%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-04-35/0_BERT/pytorch_model.bin (deflated 7%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-04-35/0_BERT/config.json (deflated 51%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-04-35/0_BERT/special_tokens_map.json (deflated 40%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-18-03/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-03-44/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-14-42/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-14-42/similarity_evaluation_results.csv (deflated 44%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-14-42/config.json (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-14-42/modules.json (deflated 51%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-14-42/1_Pooling/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-14-42/1_Pooling/config.json (deflated 47%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-14-42/0_BERT/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-14-42/0_BERT/added_tokens.json (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-14-42/0_BERT/sentence_bert_config.json (deflated 4%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-14-42/0_BERT/vocab.txt (deflated 49%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-14-42/0_BERT/pytorch_model.bin (deflated 7%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-14-42/0_BERT/config.json (deflated 51%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_09-14-42/0_BERT/special_tokens_map.json (deflated 40%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/2_Dense/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/2_Dense/pytorch_model.bin (deflated 8%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/2_Dense/config.json (deflated 22%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/similarity_evaluation_results.csv (deflated 50%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/3_Dense/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/3_Dense/pytorch_model.bin (deflated 9%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/3_Dense/config.json (deflated 23%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/config.json (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/modules.json (deflated 69%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/1_Pooling/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/1_Pooling/config.json (deflated 49%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/0_BERT/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/0_BERT/added_tokens.json (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/0_BERT/sentence_bert_config.json (deflated 4%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/0_BERT/vocab.txt (deflated 49%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/0_BERT/pytorch_model.bin (deflated 7%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/0_BERT/config.json (deflated 51%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-54-31/0_BERT/special_tokens_map.json (deflated 40%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-50-07/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-50-07/2_Dense/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-50-07/2_Dense/pytorch_model.bin (deflated 8%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-50-07/2_Dense/config.json (deflated 22%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-50-07/similarity_evaluation_results.csv (deflated 47%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-50-07/3_Dense/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-50-07/3_Dense/pytorch_model.bin (deflated 8%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-50-07/3_Dense/config.json (deflated 22%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-50-07/config.json (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-50-07/modules.json (deflated 73%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-50-07/1_Pooling/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-50-07/1_Pooling/config.json (deflated 49%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-50-07/0_BERT/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-50-07/0_BERT/added_tokens.json (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-50-07/0_BERT/sentence_bert_config.json (deflated 4%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-50-07/0_BERT/vocab.txt (deflated 49%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-12-04_08-50-07/0_BERT/pytorch_model.bin\n",
            "\n",
            "\n",
            "zip error: Interrupted (aborting)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMTz9vGGAdJv",
        "colab_type": "code",
        "outputId": "b9e1cdc4-d33a-4cb6-f87e-8445386c46e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!cp output.zip '/content/gdrive/My Drive/biobert_finetuned_sts_biomedical_mean_only.zip'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-5960452b8980>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cp output.zip '/content/gdrive/My Drive/biobert_finetuned_sts_biomedical_cnn_pool_mean_only.zip'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}