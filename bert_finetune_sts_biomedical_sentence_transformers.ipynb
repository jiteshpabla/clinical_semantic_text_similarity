{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "colab": {
      "name": "bert_finetune_sts_biomedical_sentence_transformers.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JufH3ASp0kMJ",
        "colab_type": "code",
        "outputId": "bcf095f0-1593-42f1-f5da-c1432aebebf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/e7/0a1babead1b79afabb654fbec0a052e0d833ba4205a6dfd98b1aeda9c82e/transformers-2.2.0-py3-none-any.whl (360kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 47.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 54.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.18)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 49.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.18)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=10ed617d7b2797792cf856247bd97d156acb1fb8db7e375d50201f32ee67fe90\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, regex, transformers\n",
            "Successfully installed regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "y6hfRNET0kMZ",
        "colab_type": "code",
        "outputId": "dec10097-1573-49ba-a6d8-7ae533db6a62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/17/9edba42c29fda04f2eb8597bb4de380f0f43d65e317969070c04510d93eb/sentence-transformers-0.2.3.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hCollecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 13.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.21.3)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->sentence-transformers) (0.1.83)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->sentence-transformers) (1.10.18)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->sentence-transformers) (2019.11.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->sentence-transformers) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.14.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.1.0->sentence-transformers) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.1.0->sentence-transformers) (0.2.1)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.1.0->sentence-transformers) (1.13.18)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.1.0->sentence-transformers) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.1.0->sentence-transformers) (2019.9.11)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->pytorch-transformers==1.1.0->sentence-transformers) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->pytorch-transformers==1.1.0->sentence-transformers) (0.15.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.3-cp36-none-any.whl size=57413 sha256=9d00e000f827647c2f4d82ca31094a2dd0efe9c0fdcb7a2e920e0251c6149268\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/89/88/95d8a3d4034bb77f52a6f66efdbfc623240221183dc0001c31\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: pytorch-transformers, sentence-transformers\n",
            "Successfully installed pytorch-transformers-1.1.0 sentence-transformers-0.2.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tgKRiLBw0kMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This examples trains BERT for the STSbenchmark from scratch. It generates sentence embeddings\n",
        "that can be compared using cosine-similarity to measure the similarity.\n",
        "\"\"\"\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "from sentence_transformers import SentenceTransformer,  SentencesDataset, LoggingHandler, losses, models\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers.readers import STSDataReader\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "#### Just some code to print debug information to stdout\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "                    level=logging.INFO,\n",
        "                    handlers=[LoggingHandler()])\n",
        "#### /print debug information to stdout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mqEWk1v90kMh",
        "colab_type": "code",
        "outputId": "f425fa0f-16b2-4f53-ed7a-7d385e610dc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "# Read the dataset\n",
        "train_batch_size = 16\n",
        "num_epochs = 4\n",
        "model_save_path = 'output/training_n2c2_sts_bert-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "# parameters for STSDataReader -> dataset_folder, s1_col_idx=5, s2_col_idx=6, score_col_idx=4, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, normalize_scores=True, min_score=0, max_score=5\n",
        "sts_reader = STSDataReader('', s1_col_idx=0, s2_col_idx=1, score_col_idx=2, normalize_scores=True)\n",
        "\n",
        "# Use BERT for mapping tokens to embeddings\n",
        "word_embedding_model = models.BERT('bert-base-uncased')\n",
        "\n",
        "# Apply mean pooling to get one fixed sized sentence vector\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
        "                               pooling_mode_mean_tokens=True,\n",
        "                               pooling_mode_cls_token=True,\n",
        "                               pooling_mode_max_tokens=True)\n",
        "\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 11:43:46 - https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache, downloading to /tmp/tmp36vj2b2l\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [00:00<00:00, 125316.64B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 11:43:47 - copying /tmp/tmp36vj2b2l to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
            "2019-11-28 11:43:47 - creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
            "2019-11-28 11:43:47 - removing temp file /tmp/tmp36vj2b2l\n",
            "2019-11-28 11:43:47 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
            "2019-11-28 11:43:47 - Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "2019-11-28 11:43:47 - https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache, downloading to /tmp/tmpgm1aa9gf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 440473133/440473133 [00:11<00:00, 38139167.05B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 11:43:59 - copying /tmp/tmpgm1aa9gf to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "2019-11-28 11:44:00 - creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "2019-11-28 11:44:00 - removing temp file /tmp/tmpgm1aa9gf\n",
            "2019-11-28 11:44:00 - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "2019-11-28 11:44:03 - https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmpzph6okle\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1147710.74B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 11:44:04 - copying /tmp/tmpzph6okle to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2019-11-28 11:44:04 - creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2019-11-28 11:44:04 - removing temp file /tmp/tmpzph6okle\n",
            "2019-11-28 11:44:04 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2019-11-28 11:44:04 - Use pytorch device: cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Kn7w48Ya0kMn",
        "colab_type": "code",
        "outputId": "a2356456-c639-48e7-de1c-b1f51a13497e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "source": [
        "#sts_reader = STSDataReader('', s1_col_idx=0, s2_col_idx=1, score_col_idx=2, normalize_scores=True)\n",
        "\n",
        "# Convert the dataset to a DataLoader ready for training\n",
        "logging.info(\"Read STSbenchmark train dataset\")\n",
        "train_data = SentencesDataset(sts_reader.get_examples('clinicalSTS2019.train_train60.csv'), model)\n",
        "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
        "train_loss = losses.CosineSimilarityLoss(model=model)\n",
        "\n",
        "\n",
        "logging.info(\"Read STSbenchmark dev dataset\")\n",
        "dev_data = SentencesDataset(examples=sts_reader.get_examples('clinicalSTS2019.train_validate20.csv'), model=model)\n",
        "dev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=train_batch_size)\n",
        "evaluator = EmbeddingSimilarityEvaluator(dev_dataloader)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 11:44:04 - Read STSbenchmark train dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Convert dataset: 100%|██████████| 992/992 [00:00<00:00, 1025.37it/s]\n",
            "Convert dataset:  31%|███       | 102/331 [00:00<00:00, 1015.12it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 11:44:05 - Num sentences: 992\n",
            "2019-11-28 11:44:05 - Sentences 0 longer than max_seqence_length: 0\n",
            "2019-11-28 11:44:05 - Sentences 1 longer than max_seqence_length: 0\n",
            "2019-11-28 11:44:05 - Read STSbenchmark dev dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Convert dataset: 100%|██████████| 331/331 [00:00<00:00, 1037.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 11:44:05 - Num sentences: 331\n",
            "2019-11-28 11:44:05 - Sentences 0 longer than max_seqence_length: 0\n",
            "2019-11-28 11:44:05 - Sentences 1 longer than max_seqence_length: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qB69vgWi0kMq",
        "colab_type": "code",
        "outputId": "8dd4e3b2-1c15-410f-b34a-21406aa6d00b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Configure the training. We skip evaluation in this example\n",
        "warmup_steps = math.ceil(len(train_data)*num_epochs/train_batch_size*0.1) #10% of train data for warm-up\n",
        "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 11:44:05 - Warmup-steps: 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eYRlcC2f0kMt",
        "colab_type": "code",
        "outputId": "7a05317e-95e9-4cab-9003-269baf01cfd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          evaluator=evaluator,\n",
        "          epochs=num_epochs,\n",
        "          evaluation_steps=1000,\n",
        "          warmup_steps=warmup_steps,\n",
        "          output_path=model_save_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2%|▏         | 1/62 [00:33<34:26, 33.88s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 2/62 [01:23<38:27, 38.46s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 3/62 [01:56<36:12, 36.82s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 4/62 [02:19<31:45, 32.85s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 5/62 [02:44<28:54, 30.43s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 6/62 [03:06<26:10, 28.04s/it]\u001b[A\n",
            "Iteration:  11%|█▏        | 7/62 [03:25<23:08, 25.24s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 8/62 [03:46<21:34, 23.97s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 9/62 [04:06<20:08, 22.81s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 10/62 [04:26<19:06, 22.05s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 11/62 [04:47<18:24, 21.66s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 12/62 [05:09<18:07, 21.76s/it]\u001b[A\n",
            "Iteration:  21%|██        | 13/62 [05:50<22:27, 27.51s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 14/62 [06:11<20:30, 25.64s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 15/62 [06:46<22:06, 28.22s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 16/62 [07:14<21:36, 28.19s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 17/62 [07:40<20:48, 27.74s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 18/62 [08:07<20:04, 27.38s/it]\u001b[A\n",
            "Iteration:  31%|███       | 19/62 [08:27<18:00, 25.13s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 20/62 [09:14<22:10, 31.68s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 21/62 [09:46<21:41, 31.75s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 22/62 [10:20<21:44, 32.62s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 23/62 [11:02<22:59, 35.36s/it]\u001b[A\n",
            "Iteration:  39%|███▊      | 24/62 [11:43<23:27, 37.04s/it]\u001b[A\n",
            "Iteration:  40%|████      | 25/62 [12:09<20:43, 33.60s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 26/62 [12:41<19:55, 33.22s/it]\u001b[A\n",
            "Iteration:  44%|████▎     | 27/62 [13:03<17:20, 29.73s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 28/62 [13:35<17:13, 30.41s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 29/62 [14:09<17:26, 31.72s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 30/62 [14:30<15:11, 28.48s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 31/62 [14:59<14:46, 28.61s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 32/62 [15:17<12:37, 25.24s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 33/62 [15:35<11:13, 23.22s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 34/62 [16:10<12:30, 26.81s/it]\u001b[A\n",
            "Iteration:  56%|█████▋    | 35/62 [16:36<11:56, 26.55s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 36/62 [17:24<14:12, 32.80s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 37/62 [17:41<11:43, 28.13s/it]\u001b[A\n",
            "Iteration:  61%|██████▏   | 38/62 [18:00<10:12, 25.52s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 39/62 [18:33<10:39, 27.80s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 40/62 [18:59<09:59, 27.25s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 41/62 [19:29<09:45, 27.88s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 42/62 [19:50<08:39, 25.98s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 43/62 [20:26<09:07, 28.84s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 44/62 [20:47<07:59, 26.64s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 45/62 [21:21<08:07, 28.68s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 46/62 [21:48<07:32, 28.28s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 47/62 [22:21<07:25, 29.70s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 48/62 [22:38<06:02, 25.90s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 49/62 [23:12<06:09, 28.39s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 50/62 [23:35<05:18, 26.56s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 51/62 [24:12<05:28, 29.87s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 52/62 [24:49<05:18, 31.89s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 53/62 [25:29<05:08, 34.25s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 54/62 [25:48<03:57, 29.71s/it]\u001b[A\n",
            "Iteration:  89%|████████▊ | 55/62 [26:26<03:46, 32.33s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 56/62 [26:50<02:59, 29.93s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 57/62 [27:08<02:10, 26.09s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 58/62 [27:26<01:35, 23.83s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 59/62 [27:48<01:09, 23.21s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 60/62 [28:10<00:45, 22.79s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 61/62 [28:34<00:23, 23.14s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 62/62 [29:00<00:00, 24.10s/it]\u001b[A\n",
            "Epoch:   0%|          | 0/4 [29:00<?, ?it/s]\n",
            "Convert Evaluating:   0%|          | 0/21 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 12:13:06 - Evaluation the model on  dataset after epoch 0:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Convert Evaluating:   5%|▍         | 1/21 [00:06<02:08,  6.44s/it]\u001b[A\n",
            "Convert Evaluating:  10%|▉         | 2/21 [00:19<02:38,  8.36s/it]\u001b[A\n",
            "Convert Evaluating:  14%|█▍        | 3/21 [00:32<02:54,  9.70s/it]\u001b[A\n",
            "Convert Evaluating:  19%|█▉        | 4/21 [00:39<02:34,  9.09s/it]\u001b[A\n",
            "Convert Evaluating:  24%|██▍       | 5/21 [00:44<02:04,  7.78s/it]\u001b[A\n",
            "Convert Evaluating:  29%|██▊       | 6/21 [00:52<01:59,  7.96s/it]\u001b[A\n",
            "Convert Evaluating:  33%|███▎      | 7/21 [01:06<02:17,  9.79s/it]\u001b[A\n",
            "Convert Evaluating:  38%|███▊      | 8/21 [01:16<02:05,  9.66s/it]\u001b[A\n",
            "Convert Evaluating:  43%|████▎     | 9/21 [01:20<01:37,  8.13s/it]\u001b[A\n",
            "Convert Evaluating:  48%|████▊     | 10/21 [01:29<01:30,  8.24s/it]\u001b[A\n",
            "Convert Evaluating:  52%|█████▏    | 11/21 [01:35<01:14,  7.49s/it]\u001b[A\n",
            "Convert Evaluating:  57%|█████▋    | 12/21 [01:42<01:06,  7.44s/it]\u001b[A\n",
            "Convert Evaluating:  62%|██████▏   | 13/21 [01:51<01:02,  7.83s/it]\u001b[A\n",
            "Convert Evaluating:  67%|██████▋   | 14/21 [01:58<00:53,  7.59s/it]\u001b[A\n",
            "Convert Evaluating:  71%|███████▏  | 15/21 [02:09<00:52,  8.67s/it]\u001b[A\n",
            "Convert Evaluating:  76%|███████▌  | 16/21 [02:20<00:47,  9.42s/it]\u001b[A\n",
            "Convert Evaluating:  81%|████████  | 17/21 [02:29<00:37,  9.40s/it]\u001b[A\n",
            "Convert Evaluating:  86%|████████▌ | 18/21 [02:35<00:25,  8.34s/it]\u001b[A\n",
            "Convert Evaluating:  90%|█████████ | 19/21 [02:44<00:17,  8.59s/it]\u001b[A\n",
            "Convert Evaluating:  95%|█████████▌| 20/21 [02:49<00:07,  7.42s/it]\u001b[A\n",
            "Convert Evaluating: 100%|██████████| 21/21 [02:53<00:00,  6.30s/it]\u001b[A\n",
            "Epoch:   0%|          | 0/4 [31:53<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 12:15:59 - Cosine-Similarity :\tPearson: 0.7236\tSpearman: 0.7283\n",
            "2019-11-28 12:15:59 - Manhattan-Distance:\tPearson: 0.7016\tSpearman: 0.7206\n",
            "2019-11-28 12:15:59 - Euclidean-Distance:\tPearson: 0.6999\tSpearman: 0.7206\n",
            "2019-11-28 12:15:59 - Dot-Product-Similarity:\tPearson: 0.7218\tSpearman: 0.7229\n",
            "2019-11-28 12:15:59 - Save model to output/training_n2c2_sts_bert-2019-11-28_11-43-46\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  25%|██▌       | 1/4 [31:55<1:35:45, 1915.26s/it]\n",
            "\n",
            "Epoch:  25%|██▌       | 1/4 [31:55<1:35:45, 1915.26s/it]\n",
            "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 12:16:01 - Restart data_iterator\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:   2%|▏         | 1/62 [00:20<20:26, 20.11s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 2/62 [00:40<20:08, 20.15s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 3/62 [00:57<18:54, 19.24s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 4/62 [01:12<17:23, 17.99s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 5/62 [01:47<21:48, 22.96s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 6/62 [02:10<21:30, 23.04s/it]\u001b[A\n",
            "Iteration:  11%|█▏        | 7/62 [02:49<25:35, 27.92s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 8/62 [03:15<24:39, 27.40s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 9/62 [03:52<26:33, 30.07s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 10/62 [04:39<30:39, 35.37s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 11/62 [05:12<29:16, 34.44s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 12/62 [05:29<24:27, 29.34s/it]\u001b[A\n",
            "Iteration:  21%|██        | 13/62 [05:59<24:02, 29.44s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 14/62 [06:20<21:30, 26.89s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 15/62 [06:58<23:43, 30.28s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 16/62 [07:20<21:23, 27.90s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 17/62 [07:51<21:38, 28.86s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 18/62 [08:12<19:15, 26.26s/it]\u001b[A\n",
            "Iteration:  31%|███       | 19/62 [08:34<18:02, 25.18s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 20/62 [09:12<20:21, 29.09s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 21/62 [09:29<17:15, 25.26s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 22/62 [09:59<17:46, 26.67s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 23/62 [10:46<21:24, 32.94s/it]\u001b[A\n",
            "Iteration:  39%|███▊      | 24/62 [11:23<21:34, 34.06s/it]\u001b[A\n",
            "Iteration:  40%|████      | 25/62 [11:50<19:38, 31.85s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 26/62 [12:13<17:32, 29.22s/it]\u001b[A\n",
            "Iteration:  44%|████▎     | 27/62 [12:34<15:43, 26.96s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 28/62 [13:12<17:10, 30.30s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 29/62 [13:44<16:49, 30.58s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 30/62 [14:17<16:42, 31.32s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 31/62 [14:51<16:35, 32.10s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 32/62 [15:25<16:18, 32.63s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 33/62 [15:52<15:03, 31.17s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 34/62 [16:17<13:38, 29.22s/it]\u001b[A\n",
            "Iteration:  56%|█████▋    | 35/62 [16:39<12:14, 27.22s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 36/62 [17:21<13:42, 31.63s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 37/62 [17:45<12:13, 29.34s/it]\u001b[A\n",
            "Iteration:  61%|██████▏   | 38/62 [18:11<11:16, 28.20s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 39/62 [18:36<10:29, 27.36s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 40/62 [19:02<09:52, 26.94s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 41/62 [19:25<08:57, 25.58s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 42/62 [19:55<09:02, 27.13s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 43/62 [20:30<09:15, 29.24s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 44/62 [21:06<09:27, 31.53s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 45/62 [21:36<08:44, 30.82s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 46/62 [22:10<08:32, 32.03s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 47/62 [22:35<07:25, 29.68s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 48/62 [22:57<06:24, 27.49s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 49/62 [23:37<06:44, 31.13s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 50/62 [24:08<06:15, 31.31s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 51/62 [24:28<05:06, 27.89s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 52/62 [24:45<04:04, 24.47s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 53/62 [25:21<04:10, 27.84s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 54/62 [25:41<03:23, 25.49s/it]\u001b[A\n",
            "Iteration:  89%|████████▊ | 55/62 [26:17<03:21, 28.78s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 56/62 [27:05<03:27, 34.51s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 57/62 [27:26<02:32, 30.52s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 58/62 [27:44<01:47, 26.86s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 59/62 [28:16<01:25, 28.38s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 60/62 [28:34<00:50, 25.32s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 61/62 [29:02<00:25, 25.84s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 62/62 [29:35<00:00, 28.27s/it]\u001b[A\n",
            "Epoch:  25%|██▌       | 1/4 [1:01:31<1:35:45, 1915.26s/it]\n",
            "Convert Evaluating:   0%|          | 0/21 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 12:45:37 - Evaluation the model on  dataset after epoch 1:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Convert Evaluating:   5%|▍         | 1/21 [00:06<02:05,  6.29s/it]\u001b[A\n",
            "Convert Evaluating:  10%|▉         | 2/21 [00:19<02:37,  8.30s/it]\u001b[A\n",
            "Convert Evaluating:  14%|█▍        | 3/21 [00:31<02:52,  9.57s/it]\u001b[A\n",
            "Convert Evaluating:  19%|█▉        | 4/21 [00:39<02:33,  9.03s/it]\u001b[A\n",
            "Convert Evaluating:  24%|██▍       | 5/21 [00:44<02:04,  7.76s/it]\u001b[A\n",
            "Convert Evaluating:  29%|██▊       | 6/21 [00:52<01:58,  7.88s/it]\u001b[A\n",
            "Convert Evaluating:  33%|███▎      | 7/21 [01:06<02:17,  9.85s/it]\u001b[A\n",
            "Convert Evaluating:  38%|███▊      | 8/21 [01:16<02:07,  9.83s/it]\u001b[A\n",
            "Convert Evaluating:  43%|████▎     | 9/21 [01:21<01:39,  8.26s/it]\u001b[A\n",
            "Convert Evaluating:  48%|████▊     | 10/21 [01:30<01:32,  8.42s/it]\u001b[A\n",
            "Convert Evaluating:  52%|█████▏    | 11/21 [01:36<01:16,  7.66s/it]\u001b[A\n",
            "Convert Evaluating:  57%|█████▋    | 12/21 [01:43<01:09,  7.68s/it]\u001b[A\n",
            "Convert Evaluating:  62%|██████▏   | 13/21 [01:52<01:03,  7.97s/it]\u001b[A\n",
            "Convert Evaluating:  67%|██████▋   | 14/21 [01:59<00:53,  7.67s/it]\u001b[A\n",
            "Convert Evaluating:  71%|███████▏  | 15/21 [02:10<00:52,  8.77s/it]\u001b[A\n",
            "Convert Evaluating:  76%|███████▌  | 16/21 [02:21<00:47,  9.48s/it]\u001b[A\n",
            "Convert Evaluating:  81%|████████  | 17/21 [02:30<00:37,  9.37s/it]\u001b[A\n",
            "Convert Evaluating:  86%|████████▌ | 18/21 [02:37<00:25,  8.41s/it]\u001b[A\n",
            "Convert Evaluating:  90%|█████████ | 19/21 [02:46<00:17,  8.56s/it]\u001b[A\n",
            "Convert Evaluating:  95%|█████████▌| 20/21 [02:50<00:07,  7.39s/it]\u001b[A\n",
            "Convert Evaluating: 100%|██████████| 21/21 [02:54<00:00,  6.28s/it]\u001b[A\n",
            "Epoch:  25%|██▌       | 1/4 [1:04:25<1:35:45, 1915.26s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 12:48:31 - Cosine-Similarity :\tPearson: 0.7595\tSpearman: 0.7658\n",
            "2019-11-28 12:48:31 - Manhattan-Distance:\tPearson: 0.7576\tSpearman: 0.7669\n",
            "2019-11-28 12:48:31 - Euclidean-Distance:\tPearson: 0.7553\tSpearman: 0.7656\n",
            "2019-11-28 12:48:31 - Dot-Product-Similarity:\tPearson: 0.7471\tSpearman: 0.7484\n",
            "2019-11-28 12:48:31 - Save model to output/training_n2c2_sts_bert-2019-11-28_11-43-46\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  50%|█████     | 2/4 [1:04:26<1:04:12, 1926.20s/it]\n",
            "\n",
            "Epoch:  50%|█████     | 2/4 [1:04:27<1:04:12, 1926.20s/it]\n",
            "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 12:48:32 - Restart data_iterator\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:   2%|▏         | 1/62 [00:20<21:14, 20.90s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 2/62 [00:48<22:56, 22.94s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 3/62 [01:24<26:20, 26.79s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 4/62 [01:57<27:47, 28.76s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 5/62 [02:20<25:39, 27.01s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 6/62 [02:45<24:43, 26.49s/it]\u001b[A\n",
            "Iteration:  11%|█▏        | 7/62 [03:25<27:51, 30.39s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 8/62 [03:46<24:43, 27.47s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 9/62 [04:17<25:13, 28.56s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 10/62 [05:05<29:53, 34.50s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 11/62 [05:24<25:27, 29.96s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 12/62 [05:51<24:12, 29.05s/it]\u001b[A\n",
            "Iteration:  21%|██        | 13/62 [06:29<25:57, 31.78s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 14/62 [06:54<23:47, 29.74s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 15/62 [07:15<21:04, 26.90s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 16/62 [07:45<21:17, 27.78s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 17/62 [08:12<20:44, 27.66s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 18/62 [08:48<22:09, 30.21s/it]\u001b[A\n",
            "Iteration:  31%|███       | 19/62 [09:13<20:31, 28.64s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 20/62 [09:47<21:03, 30.09s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 21/62 [10:06<18:26, 27.00s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 22/62 [10:38<18:58, 28.46s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 23/62 [11:26<22:17, 34.30s/it]\u001b[A\n",
            "Iteration:  39%|███▊      | 24/62 [11:52<20:08, 31.80s/it]\u001b[A\n",
            "Iteration:  40%|████      | 25/62 [12:20<18:56, 30.71s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 26/62 [12:45<17:16, 28.79s/it]\u001b[A\n",
            "Iteration:  44%|████▎     | 27/62 [13:14<16:50, 28.87s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 28/62 [13:51<17:51, 31.52s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 29/62 [14:20<16:51, 30.65s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 30/62 [15:02<18:10, 34.08s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 31/62 [15:28<16:25, 31.78s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 32/62 [15:48<14:06, 28.21s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 33/62 [16:18<13:48, 28.58s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 34/62 [16:36<11:52, 25.46s/it]\u001b[A\n",
            "Iteration:  56%|█████▋    | 35/62 [17:10<12:37, 28.04s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 36/62 [17:43<12:46, 29.50s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 37/62 [18:06<11:27, 27.52s/it]\u001b[A\n",
            "Iteration:  61%|██████▏   | 38/62 [18:44<12:16, 30.68s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 39/62 [19:09<11:08, 29.07s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 40/62 [19:48<11:43, 31.96s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 41/62 [20:19<11:05, 31.68s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 42/62 [20:41<09:34, 28.75s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 43/62 [21:02<08:20, 26.34s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 44/62 [21:39<08:51, 29.53s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 45/62 [22:00<07:42, 27.21s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 46/62 [22:16<06:19, 23.72s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 47/62 [22:50<06:41, 26.79s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 48/62 [23:22<06:37, 28.36s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 49/62 [23:40<05:29, 25.35s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 50/62 [24:01<04:46, 23.85s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 51/62 [24:19<04:03, 22.17s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 52/62 [24:37<03:28, 20.83s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 53/62 [25:00<03:14, 21.56s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 54/62 [25:40<03:37, 27.19s/it]\u001b[A\n",
            "Iteration:  89%|████████▊ | 55/62 [26:13<03:22, 28.92s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 56/62 [26:33<02:37, 26.33s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 57/62 [27:13<02:31, 30.37s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 58/62 [27:40<01:57, 29.34s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 59/62 [28:17<01:34, 31.57s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 60/62 [29:05<01:12, 36.49s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 61/62 [29:36<00:34, 34.94s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 62/62 [29:58<00:00, 30.86s/it]\u001b[A\n",
            "Epoch:  50%|█████     | 2/4 [1:34:25<1:04:12, 1926.20s/it]\n",
            "Convert Evaluating:   0%|          | 0/21 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 13:18:30 - Evaluation the model on  dataset after epoch 2:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Convert Evaluating:   5%|▍         | 1/21 [00:06<02:06,  6.32s/it]\u001b[A\n",
            "Convert Evaluating:  10%|▉         | 2/21 [00:19<02:37,  8.28s/it]\u001b[A\n",
            "Convert Evaluating:  14%|█▍        | 3/21 [00:31<02:52,  9.58s/it]\u001b[A\n",
            "Convert Evaluating:  19%|█▉        | 4/21 [00:39<02:34,  9.07s/it]\u001b[A\n",
            "Convert Evaluating:  24%|██▍       | 5/21 [00:44<02:04,  7.78s/it]\u001b[A\n",
            "Convert Evaluating:  29%|██▊       | 6/21 [00:52<01:59,  7.97s/it]\u001b[A\n",
            "Convert Evaluating:  33%|███▎      | 7/21 [01:07<02:18,  9.88s/it]\u001b[A\n",
            "Convert Evaluating:  38%|███▊      | 8/21 [01:16<02:08,  9.85s/it]\u001b[A\n",
            "Convert Evaluating:  43%|████▎     | 9/21 [01:21<01:39,  8.29s/it]\u001b[A\n",
            "Convert Evaluating:  48%|████▊     | 10/21 [01:30<01:32,  8.45s/it]\u001b[A\n",
            "Convert Evaluating:  52%|█████▏    | 11/21 [01:36<01:16,  7.69s/it]\u001b[A\n",
            "Convert Evaluating:  57%|█████▋    | 12/21 [01:44<01:09,  7.70s/it]\u001b[A\n",
            "Convert Evaluating:  62%|██████▏   | 13/21 [01:52<01:04,  8.04s/it]\u001b[A\n",
            "Convert Evaluating:  67%|██████▋   | 14/21 [01:59<00:54,  7.75s/it]\u001b[A\n",
            "Convert Evaluating:  71%|███████▏  | 15/21 [02:11<00:52,  8.81s/it]\u001b[A\n",
            "Convert Evaluating:  76%|███████▌  | 16/21 [02:22<00:47,  9.53s/it]\u001b[A\n",
            "Convert Evaluating:  81%|████████  | 17/21 [02:31<00:37,  9.44s/it]\u001b[A\n",
            "Convert Evaluating:  86%|████████▌ | 18/21 [02:37<00:25,  8.39s/it]\u001b[A\n",
            "Convert Evaluating:  90%|█████████ | 19/21 [02:46<00:17,  8.61s/it]\u001b[A\n",
            "Convert Evaluating:  95%|█████████▌| 20/21 [02:51<00:07,  7.44s/it]\u001b[A\n",
            "Convert Evaluating: 100%|██████████| 21/21 [02:55<00:00,  6.31s/it]\u001b[A\n",
            "Epoch:  50%|█████     | 2/4 [1:37:20<1:04:12, 1926.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 13:21:26 - Cosine-Similarity :\tPearson: 0.7884\tSpearman: 0.7896\n",
            "2019-11-28 13:21:26 - Manhattan-Distance:\tPearson: 0.7835\tSpearman: 0.7898\n",
            "2019-11-28 13:21:26 - Euclidean-Distance:\tPearson: 0.7814\tSpearman: 0.7883\n",
            "2019-11-28 13:21:26 - Dot-Product-Similarity:\tPearson: 0.7679\tSpearman: 0.7611\n",
            "2019-11-28 13:21:26 - Save model to output/training_n2c2_sts_bert-2019-11-28_11-43-46\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  75%|███████▌  | 3/4 [1:37:21<32:20, 1940.76s/it]  \n",
            "\n",
            "Epoch:  75%|███████▌  | 3/4 [1:37:21<32:20, 1940.76s/it]\n",
            "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 13:21:27 - Restart data_iterator\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:   2%|▏         | 1/62 [00:22<22:30, 22.13s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 2/62 [00:45<22:27, 22.45s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 3/62 [01:13<23:44, 24.15s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 4/62 [01:45<25:29, 26.37s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 5/62 [02:14<26:02, 27.42s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 6/62 [02:40<25:13, 27.03s/it]\u001b[A\n",
            "Iteration:  11%|█▏        | 7/62 [03:03<23:36, 25.76s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 8/62 [03:30<23:20, 25.93s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 9/62 [03:52<22:04, 25.00s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 10/62 [04:26<23:53, 27.57s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 11/62 [05:00<25:03, 29.48s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 12/62 [05:32<25:15, 30.32s/it]\u001b[A\n",
            "Iteration:  21%|██        | 13/62 [05:52<22:09, 27.13s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 14/62 [06:18<21:31, 26.90s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 15/62 [06:54<23:11, 29.60s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 16/62 [07:17<21:04, 27.48s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 17/62 [07:49<21:46, 29.03s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 18/62 [08:15<20:37, 28.13s/it]\u001b[A\n",
            "Iteration:  31%|███       | 19/62 [08:39<19:10, 26.77s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 20/62 [09:03<18:12, 26.01s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 21/62 [09:30<18:01, 26.38s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 22/62 [10:19<22:06, 33.17s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 23/62 [10:47<20:32, 31.61s/it]\u001b[A\n",
            "Iteration:  39%|███▊      | 24/62 [11:18<19:53, 31.40s/it]\u001b[A\n",
            "Iteration:  40%|████      | 25/62 [11:43<18:03, 29.28s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 26/62 [12:04<16:12, 27.01s/it]\u001b[A\n",
            "Iteration:  44%|████▎     | 27/62 [12:47<18:26, 31.63s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 28/62 [13:35<20:45, 36.62s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 29/62 [13:53<17:05, 31.07s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 30/62 [14:17<15:28, 29.03s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 31/62 [14:36<13:22, 25.88s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 32/62 [15:12<14:31, 29.05s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 33/62 [15:45<14:30, 30.03s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 34/62 [16:07<12:52, 27.59s/it]\u001b[A\n",
            "Iteration:  56%|█████▋    | 35/62 [16:28<11:36, 25.81s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 36/62 [17:09<13:03, 30.14s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 37/62 [17:40<12:41, 30.44s/it]\u001b[A\n",
            "Iteration:  61%|██████▏   | 38/62 [18:05<11:30, 28.77s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 39/62 [18:36<11:17, 29.45s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 40/62 [18:59<10:07, 27.60s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 41/62 [19:40<11:02, 31.56s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 42/62 [20:16<11:02, 33.14s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 43/62 [20:46<10:06, 31.92s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 44/62 [21:24<10:12, 34.02s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 45/62 [21:52<09:05, 32.06s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 46/62 [22:15<07:47, 29.21s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 47/62 [22:34<06:32, 26.15s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 48/62 [23:02<06:16, 26.91s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 49/62 [23:25<05:32, 25.59s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 50/62 [23:48<04:57, 24.78s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 51/62 [24:13<04:35, 25.03s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 52/62 [24:37<04:06, 24.66s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 53/62 [25:25<04:46, 31.78s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 54/62 [25:46<03:46, 28.35s/it]\u001b[A\n",
            "Iteration:  89%|████████▊ | 55/62 [26:24<03:38, 31.17s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 56/62 [26:48<02:54, 29.13s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 57/62 [27:15<02:21, 28.38s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 58/62 [27:33<01:41, 25.37s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 59/62 [27:53<01:11, 23.87s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 60/62 [28:24<00:51, 25.96s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 61/62 [29:07<00:30, 30.93s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 62/62 [29:31<00:00, 29.07s/it]\u001b[A\n",
            "Epoch:  75%|███████▌  | 3/4 [2:06:53<32:20, 1940.76s/it]\n",
            "Convert Evaluating:   0%|          | 0/21 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 13:50:59 - Evaluation the model on  dataset after epoch 3:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Convert Evaluating:   5%|▍         | 1/21 [00:06<02:07,  6.35s/it]\u001b[A\n",
            "Convert Evaluating:  10%|▉         | 2/21 [00:19<02:39,  8.37s/it]\u001b[A\n",
            "Convert Evaluating:  14%|█▍        | 3/21 [00:32<02:53,  9.64s/it]\u001b[A\n",
            "Convert Evaluating:  19%|█▉        | 4/21 [00:39<02:34,  9.11s/it]\u001b[A\n",
            "Convert Evaluating:  24%|██▍       | 5/21 [00:44<02:05,  7.82s/it]\u001b[A\n",
            "Convert Evaluating:  29%|██▊       | 6/21 [00:53<01:59,  7.97s/it]\u001b[A\n",
            "Convert Evaluating:  33%|███▎      | 7/21 [01:07<02:19,  9.96s/it]\u001b[A\n",
            "Convert Evaluating:  38%|███▊      | 8/21 [01:17<02:08,  9.91s/it]\u001b[A\n",
            "Convert Evaluating:  43%|████▎     | 9/21 [01:22<01:40,  8.35s/it]\u001b[A\n",
            "Convert Evaluating:  48%|████▊     | 10/21 [01:30<01:33,  8.50s/it]\u001b[A\n",
            "Convert Evaluating:  52%|█████▏    | 11/21 [01:37<01:17,  7.75s/it]\u001b[A\n",
            "Convert Evaluating:  57%|█████▋    | 12/21 [01:44<01:09,  7.75s/it]\u001b[A\n",
            "Convert Evaluating:  62%|██████▏   | 13/21 [01:53<01:04,  8.05s/it]\u001b[A\n",
            "Convert Evaluating:  67%|██████▋   | 14/21 [02:00<00:54,  7.75s/it]\u001b[A\n",
            "Convert Evaluating:  71%|███████▏  | 15/21 [02:11<00:53,  8.86s/it]\u001b[A\n",
            "Convert Evaluating:  76%|███████▌  | 16/21 [02:23<00:47,  9.56s/it]\u001b[A\n",
            "Convert Evaluating:  81%|████████  | 17/21 [02:32<00:37,  9.45s/it]\u001b[A\n",
            "Convert Evaluating:  86%|████████▌ | 18/21 [02:38<00:25,  8.40s/it]\u001b[A\n",
            "Convert Evaluating:  90%|█████████ | 19/21 [02:47<00:17,  8.56s/it]\u001b[A\n",
            "Convert Evaluating:  95%|█████████▌| 20/21 [02:51<00:07,  7.41s/it]\u001b[A\n",
            "Convert Evaluating: 100%|██████████| 21/21 [02:55<00:00,  6.28s/it]\u001b[A\n",
            "Epoch:  75%|███████▌  | 3/4 [2:09:49<32:20, 1940.76s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 13:53:54 - Cosine-Similarity :\tPearson: 0.7893\tSpearman: 0.7902\n",
            "2019-11-28 13:53:55 - Manhattan-Distance:\tPearson: 0.7852\tSpearman: 0.7913\n",
            "2019-11-28 13:53:55 - Euclidean-Distance:\tPearson: 0.7820\tSpearman: 0.7894\n",
            "2019-11-28 13:53:55 - Dot-Product-Similarity:\tPearson: 0.7703\tSpearman: 0.7678\n",
            "2019-11-28 13:53:55 - Save model to output/training_n2c2_sts_bert-2019-11-28_11-43-46\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch: 100%|██████████| 4/4 [2:09:50<00:00, 1943.14s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7DzTfukm0kMw",
        "colab_type": "code",
        "outputId": "00b0b806-1cbd-4bc6-dc41-1d777c56bdf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "##############################################################################\n",
        "#\n",
        "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
        "#\n",
        "##############################################################################\n",
        "\n",
        "model = SentenceTransformer(model_save_path)\n",
        "test_data = SentencesDataset(examples=sts_reader.get_examples(\"clinicalSTS2019.train_test20.csv\"), model=model)\n",
        "test_dataloader = DataLoader(test_data, shuffle=False, batch_size=train_batch_size)\n",
        "evaluator = EmbeddingSimilarityEvaluator(test_dataloader)\n",
        "model.evaluate(evaluator)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 13:53:56 - Load pretrained SentenceTransformer: output/training_n2c2_sts_bert-2019-11-28_11-43-46\n",
            "2019-11-28 13:53:56 - Load SentenceTransformer from folder: output/training_n2c2_sts_bert-2019-11-28_11-43-46\n",
            "2019-11-28 13:53:56 - loading configuration file output/training_n2c2_sts_bert-2019-11-28_11-43-46/0_BERT/config.json\n",
            "2019-11-28 13:53:56 - Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "2019-11-28 13:53:56 - loading weights file output/training_n2c2_sts_bert-2019-11-28_11-43-46/0_BERT/pytorch_model.bin\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Convert dataset:  34%|███▎      | 111/331 [00:00<00:00, 1091.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 13:53:58 - Model name 'output/training_n2c2_sts_bert-2019-11-28_11-43-46/0_BERT' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'output/training_n2c2_sts_bert-2019-11-28_11-43-46/0_BERT' is a path or url to a directory containing tokenizer files.\n",
            "2019-11-28 13:53:58 - loading file output/training_n2c2_sts_bert-2019-11-28_11-43-46/0_BERT/vocab.txt\n",
            "2019-11-28 13:53:58 - loading file output/training_n2c2_sts_bert-2019-11-28_11-43-46/0_BERT/added_tokens.json\n",
            "2019-11-28 13:53:58 - loading file output/training_n2c2_sts_bert-2019-11-28_11-43-46/0_BERT/special_tokens_map.json\n",
            "2019-11-28 13:53:58 - Use pytorch device: cpu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Convert dataset: 100%|██████████| 331/331 [00:00<00:00, 1084.00it/s]\n",
            "Convert Evaluating:   0%|          | 0/21 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 13:53:58 - Num sentences: 331\n",
            "2019-11-28 13:53:58 - Sentences 0 longer than max_seqence_length: 0\n",
            "2019-11-28 13:53:58 - Sentences 1 longer than max_seqence_length: 0\n",
            "2019-11-28 13:53:58 - Evaluation the model on  dataset:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Convert Evaluating: 100%|██████████| 21/21 [02:32<00:00,  5.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-28 13:56:31 - Cosine-Similarity :\tPearson: 0.7544\tSpearman: 0.7408\n",
            "2019-11-28 13:56:31 - Manhattan-Distance:\tPearson: 0.7258\tSpearman: 0.7316\n",
            "2019-11-28 13:56:31 - Euclidean-Distance:\tPearson: 0.7246\tSpearman: 0.7336\n",
            "2019-11-28 13:56:31 - Dot-Product-Similarity:\tPearson: 0.7424\tSpearman: 0.7278\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7408469748794851"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsee_LAZATg5",
        "colab_type": "code",
        "outputId": "733f6c77-03e4-40f8-cc17-b0857b6aa77b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "!zip -r output.zip output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: output/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-11-28_11-43-46/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-11-28_11-43-46/config.json (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-11-28_11-43-46/1_Pooling/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-11-28_11-43-46/1_Pooling/config.json (deflated 47%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-11-28_11-43-46/similarity_evaluation_results.csv (deflated 50%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-11-28_11-43-46/modules.json (deflated 51%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-11-28_11-43-46/0_BERT/ (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-11-28_11-43-46/0_BERT/config.json (deflated 51%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-11-28_11-43-46/0_BERT/vocab.txt (deflated 53%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-11-28_11-43-46/0_BERT/pytorch_model.bin (deflated 7%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-11-28_11-43-46/0_BERT/special_tokens_map.json (deflated 40%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-11-28_11-43-46/0_BERT/added_tokens.json (stored 0%)\n",
            "  adding: output/training_n2c2_sts_bert-2019-11-28_11-43-46/0_BERT/sentence_bert_config.json (deflated 4%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXgSmDIRAZZh",
        "colab_type": "code",
        "outputId": "79e42d90-9f87-4644-8c88-2eb0b437820f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMTz9vGGAdJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp output.zip '/content/gdrive/My Drive/bert_finetuned_sts_biomedical.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}